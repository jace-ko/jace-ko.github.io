---
layout: post
title:  "Zeppelin Docker Compose Yaml"
date:   2019-03-05 14:00:00 +0900
categories: tech docker
permalink: /tech/docker/:title
---

<h2>
Docker Compose File
</h2>

docker-compose.yaml

{% highlight Kconfig %}
version: "3.0"
services:
  zeppelin1:
    image: apache/zeppelin:0.8.0
    container_name: docker_zeppelin_1
    ports:
      - "8080:8080"
{% endhighlight %}

<h2>
Info
</h2>

spark가 로컬이 아닐 경우에는 spark-submit을 통해 사용하기 때문에 <br/>
spark가 zeppelin 내부에 설치가 되어 있어야 함 <br/>

up -d 이후 <br/>

Docker Container안에서 Spark를 Download( wget http://apache.mirror.cdnetworks.com/spark/spark-2.3.2/spark-2.3.2-bin-hadoop2.7.tgz ) <br/>

{% highlight Kconfig %}
docker exec -it docker_zepplein_1 bash
wget http://apache.mirror.cdnetworks.com/spark/spark-2.3.2/spark-2.3.2-bin-hadoop2.7.tgz
tar -xvf spark-2.3.2-bin-hadoop2.7.tgz
mv spark-2.3.2-bin-hadoop2.7 /spark
{% endhighlight %}

Interpreters에서 Create <br/>
Interpreter group 을 Spark로 설정
Properties에 Spark 관련 Property를 추가 <br/>
1. SPARK_HOME: /spark <br/>
2. SPARK_SUBMIT_OPTIONS: --packages datastax:spark-cassandra-connector:2.4.0-s_2.11 --conf spark.cassandra.connection.host=192.168.56.101 <br/>

{% highlight Kconfig %}

SPARK_HOME: /spark
{% endhighlight %}